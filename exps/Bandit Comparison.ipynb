{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "import plotting_utilities\n",
    "\n",
    "from rllib.agent.bandit.gp_ucb_agent import GPUCBPolicy\n",
    "from rllib.environment.bandit_environment import BanditEnvironment\n",
    "from rllib.reward.gp_reward import GPBanditReward\n",
    "from rllib.util import rollout_agent\n",
    "from rllib.util.gaussian_processes import ExactGP\n",
    "from rllib.util.gaussian_processes.utilities import add_data_to_gp\n",
    "\n",
    "plotting_utilities.set_figure_params(serif=True)\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(x: torch.Tensor, model: gpytorch.models.GP, num_samples: int, ax: mpl.axes.Axes) -> None:\n",
    "    \"\"\"Plot 1-D GP.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: points to plot.\n",
    "    model: GP model.\n",
    "    num_samples: number of random samples from gp.\n",
    "    ax: axes where to plot the plot.\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = model(x)\n",
    "        mean = pred.mean.numpy()\n",
    "        error = 2 * pred.stddev.numpy()\n",
    "        true_values = objective(None, x, None)[0].numpy()\n",
    "\n",
    "    # Plot gp prediction\n",
    "    ax.fill_between(x, mean - error, mean + error, lw=0, alpha=0.4, color='C0')\n",
    "        \n",
    "    # Plot mean\n",
    "    ax.plot(x, mean, color='C0')\n",
    "    \n",
    "    # Plot ground-truth\n",
    "    ax.plot(x, true_values, '--', color='k')\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(model.train_inputs[0].numpy(),\n",
    "            model.train_targets.numpy(),\n",
    "            'x', markeredgewidth=2, markersize=5, color='C1')\n",
    "\n",
    "    # Plot samples.\n",
    "    for _ in range(num_samples):\n",
    "        ax.plot(x.numpy(), pred.sample().numpy())\n",
    "    \n",
    "    ax.set_xlim(x[0], x[-1])\n",
    "    ax.set_ylim(-2.1, 2.1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(r\"Inputs $\\theta$\")\n",
    "    ax.set_ylabel(r\"$J(\\theta)$\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "X = torch.tensor([-1., 1., 2.5, 4., 6])\n",
    "Y = 2 * torch.tensor([-0.5, 0.3, -0.2, .6, -0.5])\n",
    "\n",
    "NUM_POINTS = 1000\n",
    "x = torch.linspace(-1, 6, NUM_POINTS)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood.noise_covar.noise = 0.1 ** 2\n",
    "objective_function = ExactGP(X, Y, likelihood)\n",
    "objective_function.eval()\n",
    "objective = GPBanditReward(objective_function)\n",
    "environment = BanditEnvironment(objective, x_min=x[[0]].numpy(), x_max=x[[-1]].numpy())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5.5/2, 1.5)\n",
    "with torch.no_grad():\n",
    "    ax.plot(x, objective_function(x).mean, 'k--')\n",
    "ax.set_xlabel(r\"Inputs $\\theta$\")\n",
    "ax.set_ylabel(r\"$J(\\theta)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_policy(beta=2.0, noisy=False):\n",
    "    x0 = x[x > 0.2][[0]].unsqueeze(-1)\n",
    "    y0 = objective(None, x0, None)[0].type(torch.get_default_dtype())\n",
    "    gp = ExactGP(x0, y0, likelihood)\n",
    "    policy = GPUCBPolicy(gp, x, beta=beta, noisy=noisy)\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_new_policy()\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5.5/2, 1.5)\n",
    "plot_gp(x, policy.gp, num_samples=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without optimism (beta=0) and with no noise in the optimization process, GP-UCB gets stuck evaluating the first observation with expected value larger than the mean. By design, this is the case for the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_new_policy(beta=0.0)\n",
    "\n",
    "for i in range(10):\n",
    "    query_x = policy(None)[0]\n",
    "    _, query_y, _, _ = environment.step(query_x)\n",
    "    add_data_to_gp(policy.gp, query_x.unsqueeze(-1), torch.tensor(query_y, dtype=torch.float))\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5.5 / 2.2, 2)\n",
    "plot_gp(x, policy.gp, num_samples=0, ax=ax)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.savefig('bandit_exploration_beta_0_b.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the optimization process is noisy, this method at least converges to a local optimum by exploiting gradient information provided by the slighly randomized evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_new_policy(beta=0.0, noisy=True)\n",
    "\n",
    "for i in range(10):\n",
    "    query_x = policy(None)[0]\n",
    "    _, query_y, _, _ = environment.step(query_x)\n",
    "    add_data_to_gp(policy.gp, query_x.unsqueeze(-1), torch.tensor(query_y, dtype=torch.float))\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5.5 / 2.2, 2)\n",
    "plot_gp(x, policy.gp, num_samples=0, ax=ax)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.savefig('bandit_exploration_beta_noisy_b.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optimism (beta=2), GP-UCB converges to the global optimum as one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_new_policy(beta=2.0)\n",
    "\n",
    "for i in range(10):\n",
    "    query_x = policy(None)[0]\n",
    "    _, query_y, _, _ = environment.step(query_x)\n",
    "    add_data_to_gp(policy.gp, query_x.unsqueeze(-1), torch.tensor(query_y, dtype=torch.float))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5.5 / 2.2, 2)\n",
    "plot_gp(x, policy.gp, num_samples=0, ax=ax)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.savefig('bandit_exploration_beta_2_b.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
